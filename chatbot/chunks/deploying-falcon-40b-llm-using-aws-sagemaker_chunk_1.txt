A big part of this is improving the speed and efficiency of machine learning models’ understanding of human language via Natural Language Processing (NLP). About Deep Learning Deep learning refers to a subset of machine learning that focuses on training artificial neural networks with multiple layers to learn hierarchical representations of data. These neural networks, known as deep neural networks, are capable of automatically extracting intricate features and patterns from complex data like language. In the field of natural language processing, deep learning has been instrumental in transforming the way computers understand and process human language. By leveraging deep neural networks, researchers and developers have achieved significant advancements in various language-related tasks, such as speech recognition systems and transcription. This has paved the way for voice-controlled devices, interactive voice response systems, and voice assistants like Siri and Alexa. Falcon: An Innovative LLM To understand Falcon, it is important to first know Large Language Models, commonly known as LLMs, which are machine learning models trained to understand large texts and generate or predict new content. Falcon is an open-source LLM launched by the Technology Innovation Institute characterized by top performance in the field. It has significant applications in the fields of language comprehension and text/ response generation. This model is offered in two sizes: Falcon-7B, which typically requires only a single GPU, and Falcon-40B, which requires multiple GPUs and will be used in this article. In this tutorial, we will be diving into harnessing the power Falcon-40B using Amazon SageMaker to optimize the hosting, deployment, and use of such models. SageMaker features large model inference deep learning containers (LMI DLCs) which are optimized to handle such models that require high memory and complex computations. On the backend, these containers can distribute model parameters and computations between several GPUs. What is SageMaker? Amazon SageMaker is a fully managed service by AWS for building, training, and deploying machine learning models. It provides a comprehensive set of tools and capabilities, including data preparation, model training, and deployment. With SageMaker, users can create and manage training environments, choose from built-in algorithms or bring their own models, and benefit from automated model tuning and distributed training. It simplifies deployment with managed hosting and scalability for real-time inference, while offering built-in monitoring and logging features. In short, SageMaker accelerates the development and deployment of intelligent applications with its ease of use and scalability. How about Hugging Face? Hugging Face is a company responsible for the development of the open-source library “Transformers”, which provides a wide range of pre-trained models which includes various NLP tasks, like large language models with impressive performance. Hugging Face also offers the Hugging Face Model Hub, a platform to find, share, download, and edit pre-trained models. This tutorial will assume your model is on Hugging Face to benefit from Amazon SageMaker’s new integration with this platform. Overview The main steps to take your model from trained and stored to deployed and usable in Amazon SageMaker are: Reference the model: Define your model in Amazon SageMaker, specifying the necessary information such as the container image, model artifacts, and any configuration specifications. Create an endpoint configuration: Configure the endpoint settings, including the instance type, instance count, etc. and be sure to specify the LMI DLC as the container image for the endpoint. Deploy the endpoint: Start the deployment of your model by creating an Amazon SageMaker endpoint using the previously defined endpoint configuration. Cleanup: Remove resources you no longer need Before starting Using SageMaker offers plenty of options to fine-tune your model deployment, which you can use to maximize the throughput and inference power of your model. First and foremost, choose your instance size and type based on your model’s size, parameters and memory requirements. For example, most models require GPU acceleration for enhanced performance and inference speed and SageMaker recommends ml.g4 or ml.p3, which are GPU-backed instances. To increase throughput, SageMaker features a batch transform functionality, which allows you to perform inference on your model’s input data offline and process the data in parallel. Moreover, to further reduce your model’s inference latency, you can leverage the Result Caching feature to store frequently accessed results and Result Aggregation to collect multiple requests into a batch. After deploying your endpoint to use your model, it is best to configure automatic scaling provided by SageMaker to dynamically adjust the number of instances based on the inference traffic. Walkthrough Note: To write and run commands and code, you can use SageMaker notebook instances or SageMaker Studio notebooks. Find Amazon SageMaker After logging into your account on the AWS Management Console, search for the SageMaker service. If it is your first time using this service, you will be prompted to create a SageMaker domain. Set Up a SageMaker Domain to be able to use SageMaker Notebooks Create the necessary roles to be able to access other AWS services through SageMaker, particulary S3. Choose the VPC within which your SageMaker Domain will be defined. While AWS sets up your SageMaker domain, its state will be pending, as shown below: When the domain is in service, you can now use the Notebook Studio to write your code and create your files. Open your SageMaker Studio with the same user profile as your SageMaker domain: Open a notebook and begin coding: Import the sagemaker dependencies import sagemaker from sagemaker import image_uris import boto3 #library to access S3 and SageMaker programmatically import json Define your SageMaker session sage_session sagemaker.session.Session() role sagemaker.get_execution_role() region sage_session._region_name maker boto3.client("sagemaker") runtime boto3.client("sagemaker-runtime") Define & Upload your model to S3 Create your local folder !mkdir -p code_container Create the configuration file specifying the falcon-40B model %%writefile ./code_container/serving.properties enginePython option.model_id'tiiuae/falcon-40b' option.tensor_parallel_degree4 It should contain the id of the Hugging Face model to be uploaded to s3, the number of GPUs required on the instance. Create a text file to contain PyTorch package version to be installed %%writefile ./code_container/requirements.txt einops torch2.0.1 Create the model.py file in the same directory: %%writefile ./code_container/model.py from djl_python import Input, Output import torch from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer import os predictor None def get_model(properties): model_name 'tiiuae/falcon-40b' local_rank int(os.getenv("LOCAL_RANK", "0")) model AutoModelForCausalLM.from_pretrained( model_name, low_cpu_mem_usageTrue, trust_remote_codeTrue, torch_dtypetorch.bfloat16, device_map"auto", ) tokenizer AutoTokenizer.from_pretrained(model_name) generator pipeline( task"text-generation", modelmodel, tokenizertokenizer, device_map"auto" ) return generator def handle(inputs: Input) None: global predictor if not predictor: predictor get_model(inputs.get_properties()) if inputs.is_empty(): return None data inputs.get_as_json() text data["text"] text_length data["text_length"] outputs predictor(text, do_sampleTrue, min_lengthtext_length, max_lengthtext_length) result {"outputs": outputs} return Output().add_as_json(result) This file contains integral functions which will deal with text generation and use the model. The get_model() function initializes and returns a generator object for text generation using the tiiuae/falcon-40b model. It loads the model using AutoModelForCausalLM. from_pretrained() and the tokenizer using AutoTokenizer.