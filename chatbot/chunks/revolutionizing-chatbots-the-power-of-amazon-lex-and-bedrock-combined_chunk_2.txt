Launched a t2.micro EC2 instance running Ubuntu. Granted the EC2 instance permissions to write to our S3 bucket where custom layers are stored. Ran the following instructions to update the environment if needed and create the necessary dependency structure. sudo apt-get update # Update environment Python3 -V # Check python version to check if it needs updating pip3 # Check if pip3 is installed sudo apt install python3-pip # Install pip3 if needed zip # Check if zip is installed sudo apt install zip # Install zip if needed sudo apt install awscli # Install AWS Command Line Interface mkdir -p build/python/lib/python3.X/site-packages # Create dependency structure, X is the python version. We're using X 12 4. Installed all the packages we wanted for our layer on the EC2 instance. pip3 install requests -t build/python/lib/python3.X/site-packages pip3 install boto3 -t build/python/lib/python3.X/site-packages pip3 install beautifulsoup4 -t build/python/lib/python3.X/site-packages 5. Zipped the packages and uploaded them to S3. cd build/ # Go to the build folder zip -r layer_dependencies.zip . # Zip current directory aws s3 cp layer_dependencies.zip s3://lambda-layer-dependencies-s3-bucket 6. As we uploaded our dependencies zipped on S3, we created a new lambda layer and uploaded the zipped file there. 7. Afterwards, we attached our new lambda layer to our lambda function. Following that, we aimed to automate the process of updating the data stored in the S3 bucket every six hours to ensure it remains current with any content updates made to the website. To achieve this, we utilized EventBridge scheduling, specifying the payload as our base URL and the bucket name. As demonstrated below: After completing this step, we verified the S3 bucket following the execution of the function triggered by the schedule to ensure everything was functioning well. As depicted in the image below, all the web pages from our website are now successfully stored within the S3 bucket. Now, we can proceed to the next stage of creating the Amazon Bedrock knowledge base. Establishing the Amazon Bedrock Knowledge Base We began by providing the necessary details for the knowledge base. 2. Next, we configured our data source, setting our previously created S3 bucket as illustrated below. 3. Lastly, we selected the appropriate embeddings model and configured the vector store to best suit our data. With everything set up and configured, we commenced the development of our Amazon Lex bot. Building the Amazon Lex Bot We started this step by choosing amazon lex bot’s creation method to be Descriptive Bot Builder , which is a newly added feature to amazon lex that allow us to provide description in natural language to have lex generate a bot for us using large language models. Upon configuration completion, we were directed to a dashboard featuring a section labeled “intents.” An intent, as we learned, serves as a mechanism to execute actions in response to natural language user input. To harness the capabilities of GenAI with Lex effectively, we realized the importance of utilizing a bedrock-powered built-in intent. These intents leverage generative AI to fulfill FAQ requests by accessing authorized knowledge content. In the QnA Configuration section, we selected the knowledge base we had previously created. (You can locate the Knowledge Base ID in the Amazon Bedrock console) To advance with bot creation, we had to establish at least one custom intent. We opted for the HelloIntent to reply to greetings (feel free to add whatever you feel needed). To create your own intents, it’s essential to understand two key concepts: Slot Types: These define the types of data your bot expects to receive from the user. For example, a “Date” slot type would expect input like “today” or “next Monday.” Utterances: These are examples of what users might say to trigger a specific intent. For the HelloIntent , utterances could include “hello,” “hi there,” or “good morning.” Thanks to Lex’s generative AI features powered by Amazon Bedrock, such as assisted slot resolution and sample utterance generation, we can easily add and configure slots and utterances for our intents. These advanced capabilities streamline the process, making it easier to define the various elements of our bot’s conversational flow.